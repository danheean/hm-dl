{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112f014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MPS (Apple Silicon GPU) available\n",
      "ğŸ¯ Keras backend: torch\n"
     ]
    }
   ],
   "source": [
    "# ë°˜ë“œì‹œ ì²« ë²ˆì§¸ ì…€ì—ì„œ ë°±ì—”ë“œ ì„¤ì •\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "\n",
    "# PyTorch MPS í™•ì¸\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"âœ… MPS (Apple Silicon GPU) available\")\n",
    "else:\n",
    "    print(\"âŒ MPS not available, using CPU\")\n",
    "\n",
    "# Keras import (ë°±ì—”ë“œ í™•ì¸)\n",
    "import keras\n",
    "print(f\"ğŸ¯ Keras backend: {keras.backend.backend()}\")\n",
    "\n",
    "# TensorFlowëŠ” importí•˜ì§€ ì•ŠìŒ!\n",
    "import keras\n",
    "import keras_hub\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6794b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851d2508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 15:12:41.581550: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Max\n",
      "2025-08-20 15:12:41.581691: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2025-08-20 15:12:41.581694: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2025-08-20 15:12:41.581702: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-08-20 15:12:41.581711: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"llama_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"llama_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                                                  </span>â”ƒ<span style=\"font-weight: bold\">                                   Config </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ llama_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LlamaTokenizer</span>)                              â”‚                       Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">32,000</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ llama_tokenizer (\u001b[38;5;33mLlamaTokenizer\u001b[0m)                              â”‚                       Vocab size: \u001b[38;5;34m32,000\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"llama_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"llama_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ llama_backbone_1              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)        â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">6,738,415,616</span> â”‚ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LlamaBackbone</span>)               â”‚                           â”‚                 â”‚ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_embedding               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32000</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144,000</span> â”‚ llama_backbone_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ llama_backbone_1              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)        â”‚   \u001b[38;5;34m6,738,415,616\u001b[0m â”‚ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        â”‚\n",
       "â”‚ (\u001b[38;5;33mLlamaBackbone\u001b[0m)               â”‚                           â”‚                 â”‚ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_embedding               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32000\u001b[0m)       â”‚     \u001b[38;5;34m262,144,000\u001b[0m â”‚ llama_backbone_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,738,415,616</span> (25.10 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,738,415,616\u001b[0m (25.10 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,738,415,616</span> (25.10 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,738,415,616\u001b[0m (25.10 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama2 = keras_hub.models.LlamaCausalLM.from_preset('llama2_7b_en')\n",
    "llama2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65f9168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.2\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c6e7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay hungry, stay hungry!\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "llama2.compile(sampler=sampler)\n",
    "\n",
    "text = llama2.generate('stay hungry, stay', max_length=100)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c199dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7952,  9074, 14793, 29892,  7952], device='mps:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer = keras_hub.models.LlamaTokenizer.from_preset('llama2_7b_en')\n",
    "\n",
    "token_ids = llama_tokenizer.tokenize('stay hungry, stay')\n",
    "\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e969816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–stay â–hun gry , â–stay "
     ]
    }
   ],
   "source": [
    "# MPS í…ì„œë¥¼ CPUë¡œ ì´ë™ í›„ í† í° ë³€í™˜\n",
    "for ids in token_ids.cpu():\n",
    "    print(llama_tokenizer.id_to_token(ids), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670d18e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay foolish.\n",
      "\n",
      "i'm excited for 2019! i'm looking forward to being more consistent with my\n"
     ]
    }
   ],
   "source": [
    "gemma = keras_hub.models.GemmaCausalLM.from_preset('gemma2_2b_en')\n",
    "\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gemma.compile(sampler=sampler)\n",
    "\n",
    "text = gemma.generate('stay hungry, stay', max_length=30)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1151359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15043, 29892, 22172], device='mps:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.tokenize('Hello, hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd28be13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stay hungry, stay'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.detokenize(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edef841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40200d763e964cc9ad7fdbb085f586eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "llama3_pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "llama3_pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9acf3c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "â”œâ”€LlamaModel: 1-1                                       --\n",
       "â”‚    â””â”€Embedding: 2-1                                   525,336,576\n",
       "â”‚    â””â”€ModuleList: 2-2                                  --\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-1                      218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-2                      218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-3                      218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-4                      218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-5                      218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-6                      218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-7                      218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-8                      218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-9                      218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-10                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-11                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-12                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-13                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-14                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-15                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-16                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-17                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-18                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-19                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-20                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-21                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-22                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-23                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-24                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-25                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-26                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-27                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-28                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-29                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-30                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-31                     218,112,000\n",
       "â”‚    â”‚    â””â”€LlamaDecoderLayer: 3-32                     218,112,000\n",
       "â”‚    â””â”€LlamaRMSNorm: 2-3                                4,096\n",
       "â”‚    â””â”€LlamaRotaryEmbedding: 2-4                        --\n",
       "â”œâ”€Linear: 1-2                                           525,336,576\n",
       "================================================================================\n",
       "Total params: 8,030,261,248\n",
       "Trainable params: 8,030,261,248\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(llama3_pipe.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffb296fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay foolish\n",
      "The most important thing is to keep the most important people in your life.\n",
      "When you're in a relationship, it's not just about the person you're with, but about the people around you.\n",
      "The most important thing is to keep the most important people in your life.\n",
      "When you're in a relationship, it's not just about the person you're with, but about the people around you.\n",
      "The most important thing is to keep the most important people in your\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"stay hungry, stay foolish\\nThe most important thing is to keep the most important people in your life.\\nWhen you're in a relationship, it's not just about the person you're with, but about the people around you.\\nThe most important thing is to keep the most important people in your life.\\nWhen you're in a relationship, it's not just about the person you're with, but about the people around you.\\nThe most important thing is to keep the most important people in your\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_pipe.model.generation_config.pad_token_id = llama3_pipe.model.generation_config.eos_token_id\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "result = llama3_pipe('stay hungry, stay', max_length=100, truncation=True)\n",
    "\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb1d6b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llama3_pipe.model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c5126fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'ë´„ì´ ì˜¤ë©´(2018) 1.0\\nCast : Kim Go-E'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "llama3_pipe('ë´„ì´ ì˜¤ë©´', max_length=20, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3960ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df752290618c48c1acc4f0d70b7bc9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë´„ì´ ì˜¤ë©´ ë§ì€ ì‚¬ëŒë“¤ì´ ìì—° ì†ìœ¼ë¡œ ë‚˜ì•„ê°€ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. ì‚°ì±…, ìº í•‘, ë‚šì‹œ ë“± ë‹¤ì–‘í•œ Outdoor í™œë™ì„ ì¦ê¸°ê¸° ìœ„í•´ì„œëŠ” ì•ˆì „ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. íŠ¹íˆ, ë´„ì² ì€ ë‚ ì”¨ê°€ ê¸‰ê²©íˆ ë³€í•  ìˆ˜ ìˆëŠ” ì‹œê¸°ì´ë¯€ë¡œ, ì¤€ë¹„ê°€ ì˜ ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì•ˆì „ì„ ìœ„í•œ ëª‡ ê°€ì§€ íŒì„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
      "\n",
      "### 1. ë‚ ì”¨ ì˜ˆë³´ í™•ì¸í•˜ê¸°\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'ë´„ì´ ì˜¤ë©´ ë§ì€ ì‚¬ëŒë“¤ì´ ìì—° ì†ìœ¼ë¡œ ë‚˜ì•„ê°€ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. ì‚°ì±…, ìº í•‘, ë‚šì‹œ ë“± ë‹¤ì–‘í•œ Outdoor í™œë™ì„ ì¦ê¸°ê¸° ìœ„í•´ì„œëŠ” ì•ˆì „ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. íŠ¹íˆ, ë´„ì² ì€ ë‚ ì”¨ê°€ ê¸‰ê²©íˆ ë³€í•  ìˆ˜ ìˆëŠ” ì‹œê¸°ì´ë¯€ë¡œ, ì¤€ë¹„ê°€ ì˜ ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì•ˆì „ì„ ìœ„í•œ ëª‡ ê°€ì§€ íŒì„ ì†Œê°œí•©ë‹ˆë‹¤.\\n\\n### 1. ë‚ ì”¨ ì˜ˆë³´ í™•ì¸í•˜ê¸°\\n'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "llama3_bllossom = pipeline('text-generation', model='MLP-KTLim/llama-3-Korean-Bllossom-8B')\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "result = llama3_bllossom('ë´„ì´ ì˜¤ë©´', max_length=100, truncation=True)\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ce6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
