{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112f014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MPS (Apple Silicon GPU) available\n",
      "🎯 Keras backend: torch\n"
     ]
    }
   ],
   "source": [
    "# 반드시 첫 번째 셀에서 백엔드 설정\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "\n",
    "# PyTorch MPS 확인\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"✅ MPS (Apple Silicon GPU) available\")\n",
    "else:\n",
    "    print(\"❌ MPS not available, using CPU\")\n",
    "\n",
    "# Keras import (백엔드 확인)\n",
    "import keras\n",
    "print(f\"🎯 Keras backend: {keras.backend.backend()}\")\n",
    "\n",
    "# TensorFlow는 import하지 않음!\n",
    "import keras\n",
    "import keras_hub\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6794b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851d2508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 15:12:41.581550: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Max\n",
      "2025-08-20 15:12:41.581691: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2025-08-20 15:12:41.581694: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2025-08-20 15:12:41.581702: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-08-20 15:12:41.581711: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"llama_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"llama_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ llama_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LlamaTokenizer</span>)                              │                       Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">32,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ llama_tokenizer (\u001b[38;5;33mLlamaTokenizer\u001b[0m)                              │                       Vocab size: \u001b[38;5;34m32,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"llama_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"llama_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ llama_backbone_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">6,738,415,616</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LlamaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32000</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144,000</span> │ llama_backbone_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ llama_backbone_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)        │   \u001b[38;5;34m6,738,415,616\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mLlamaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32000\u001b[0m)       │     \u001b[38;5;34m262,144,000\u001b[0m │ llama_backbone_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,738,415,616</span> (25.10 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,738,415,616\u001b[0m (25.10 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,738,415,616</span> (25.10 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,738,415,616\u001b[0m (25.10 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama2 = keras_hub.models.LlamaCausalLM.from_preset('llama2_7b_en')\n",
    "llama2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65f9168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.2\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c6e7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay hungry, stay hungry!\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "llama2.compile(sampler=sampler)\n",
    "\n",
    "text = llama2.generate('stay hungry, stay', max_length=100)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c199dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7952,  9074, 14793, 29892,  7952], device='mps:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer = keras_hub.models.LlamaTokenizer.from_preset('llama2_7b_en')\n",
    "\n",
    "token_ids = llama_tokenizer.tokenize('stay hungry, stay')\n",
    "\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e969816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁stay ▁hun gry , ▁stay "
     ]
    }
   ],
   "source": [
    "# MPS 텐서를 CPU로 이동 후 토큰 변환\n",
    "for ids in token_ids.cpu():\n",
    "    print(llama_tokenizer.id_to_token(ids), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670d18e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay foolish.\n",
      "\n",
      "i'm excited for 2019! i'm looking forward to being more consistent with my\n"
     ]
    }
   ],
   "source": [
    "gemma = keras_hub.models.GemmaCausalLM.from_preset('gemma2_2b_en')\n",
    "\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gemma.compile(sampler=sampler)\n",
    "\n",
    "text = gemma.generate('stay hungry, stay', max_length=30)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1151359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15043, 29892, 22172], device='mps:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.tokenize('Hello, hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd28be13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stay hungry, stay'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.detokenize(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edef841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40200d763e964cc9ad7fdbb085f586eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "llama3_pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "llama3_pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9acf3c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForCausalLM                                        --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   525,336,576\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-9                      218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-10                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-11                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-12                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-13                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-14                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-15                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-16                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-17                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-18                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-19                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-20                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-21                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-22                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-23                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-24                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-25                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-26                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-27                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-28                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-29                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-30                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-31                     218,112,000\n",
       "│    │    └─LlamaDecoderLayer: 3-32                     218,112,000\n",
       "│    └─LlamaRMSNorm: 2-3                                4,096\n",
       "│    └─LlamaRotaryEmbedding: 2-4                        --\n",
       "├─Linear: 1-2                                           525,336,576\n",
       "================================================================================\n",
       "Total params: 8,030,261,248\n",
       "Trainable params: 8,030,261,248\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(llama3_pipe.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffb296fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay foolish\n",
      "The most important thing is to keep the most important people in your life.\n",
      "When you're in a relationship, it's not just about the person you're with, but about the people around you.\n",
      "The most important thing is to keep the most important people in your life.\n",
      "When you're in a relationship, it's not just about the person you're with, but about the people around you.\n",
      "The most important thing is to keep the most important people in your\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"stay hungry, stay foolish\\nThe most important thing is to keep the most important people in your life.\\nWhen you're in a relationship, it's not just about the person you're with, but about the people around you.\\nThe most important thing is to keep the most important people in your life.\\nWhen you're in a relationship, it's not just about the person you're with, but about the people around you.\\nThe most important thing is to keep the most important people in your\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_pipe.model.generation_config.pad_token_id = llama3_pipe.model.generation_config.eos_token_id\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "result = llama3_pipe('stay hungry, stay', max_length=100, truncation=True)\n",
    "\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb1d6b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llama3_pipe.model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c5126fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '봄이 오면(2018) 1.0\\nCast : Kim Go-E'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "llama3_pipe('봄이 오면', max_length=20, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3960ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df752290618c48c1acc4f0d70b7bc9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "봄이 오면 많은 사람들이 자연 속으로 나아가고 싶어 합니다. 산책, 캠핑, 낚시 등 다양한 Outdoor 활동을 즐기기 위해서는 안전을 최우선으로 고려해야 합니다. 특히, 봄철은 날씨가 급격히 변할 수 있는 시기이므로, 준비가 잘 되어 있어야 합니다. 안전을 위한 몇 가지 팁을 소개합니다.\n",
      "\n",
      "### 1. 날씨 예보 확인하기\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '봄이 오면 많은 사람들이 자연 속으로 나아가고 싶어 합니다. 산책, 캠핑, 낚시 등 다양한 Outdoor 활동을 즐기기 위해서는 안전을 최우선으로 고려해야 합니다. 특히, 봄철은 날씨가 급격히 변할 수 있는 시기이므로, 준비가 잘 되어 있어야 합니다. 안전을 위한 몇 가지 팁을 소개합니다.\\n\\n### 1. 날씨 예보 확인하기\\n'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "llama3_bllossom = pipeline('text-generation', model='MLP-KTLim/llama-3-Korean-Bllossom-8B')\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "result = llama3_bllossom('봄이 오면', max_length=100, truncation=True)\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ce6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
