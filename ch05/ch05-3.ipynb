{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8352a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MPS (Apple Silicon GPU) available\n",
      "🎯 Keras backend: torch\n"
     ]
    }
   ],
   "source": [
    "# 반드시 첫 번째 셀에서 백엔드 설정\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "# TensorFlow 완전 차단\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# PyTorch MPS 확인\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"✅ MPS (Apple Silicon GPU) available\")\n",
    "    torch.mps.set_per_process_memory_fraction(0.8)  # GPU 메모리 제한\n",
    "else:\n",
    "    print(\"❌ MPS not available, using CPU\")\n",
    "\n",
    "# Keras import (백엔드 확인)\n",
    "import keras\n",
    "print(f\"🎯 Keras backend: {keras.backend.backend()}\")\n",
    "\n",
    "# TensorFlow는 import하지 않음!\n",
    "import keras_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516ff87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경변수 로드만\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e3f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma model loaded with PyTorch backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 19:58:19.038206: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Max\n",
      "2025-08-19 19:58:19.038282: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2025-08-19 19:58:19.038287: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2025-08-19 19:58:19.038295: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-08-19 19:58:19.038305: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch 백엔드로 Gemma 모델 로딩\n",
    "gemma = keras_hub.models.GemmaCausalLM.from_preset('gemma_2b_en')\n",
    "\n",
    "print(\"Gemma model loaded with PyTorch backend\")\n",
    "# gemma.summary()  # 메모리 절약을 위해 주석 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e903d790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay foolish\n",
      "\n",
      "<h1>#stayhungrystayfoolish</h1>\n",
      "\n",
      "<strong>What does this\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gemma.compile(sampler=sampler)\n",
    "\n",
    "# PyTorch 백엔드로 텍스트 생성 (MPS GPU 자동 사용)\n",
    "text = gemma.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a6f64d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "봄이 오면서 건조한 흐린 날씨가 오고 있습니다. \n"
     ]
    }
   ],
   "source": [
    "text = gemma.generate('봄이 오면', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb63b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a20ce6d1dc744f4b39022ce48db989a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '봄이 오면서 겨울이 빨리 지나가는 것 같아'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "gemma_pipe = pipeline('text-generation', model='beomi/gemma-ko-2b')\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "text = gemma_pipe('봄이 오면', max_length=20, truncation=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110eec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay foolish.\n",
      "\n",
      "i'm excited for 2019! i'm looking forward to being more consistent with my\n"
     ]
    }
   ],
   "source": [
    "gemma = keras_hub.models.GemmaCausalLM.from_preset('gemma2_2b_en')\n",
    "\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gemma.compile(sampler=sampler)\n",
    "\n",
    "text = gemma.generate('stay hungry, stay', max_length=30)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb432e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay foolish\n",
      "\n",
      "I have been trying to get the past few months to go back\n"
     ]
    }
   ],
   "source": [
    "#gemma = keras_hub.models.Gemma3CausalLM.from_preset('gemma3_12b')\n",
    "gemma = keras_hub.models.Gemma3CausalLM.from_preset('gemma3_4b')\n",
    "\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gemma.compile(sampler=sampler)\n",
    "\n",
    "text = gemma.generate('stay hungry, stay', max_length=20)\n",
    "print(text)\n",
    "\n",
    "# gemma.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532afc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay hungry<start_of_image><start_of_image>　　北京时间7月27\n"
     ]
    }
   ],
   "source": [
    "gemma = keras_hub.models.Gemma3CausalLM.from_preset('gemma3_270m')\n",
    "\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gemma.compile(sampler=sampler)\n",
    "\n",
    "text = gemma.generate('stay hungry, stay', max_length=20)\n",
    "print(text)\n",
    "\n",
    "# gemma.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
