{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8352a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MPS (Apple Silicon GPU) available\n",
      "ğŸ¯ Keras backend: torch\n"
     ]
    }
   ],
   "source": [
    "# ë°˜ë“œì‹œ ì²« ë²ˆì§¸ ì…€ì—ì„œ ë°±ì—”ë“œ ì„¤ì •\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "# TensorFlow ì™„ì „ ì°¨ë‹¨\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# PyTorch MPS í™•ì¸\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"âœ… MPS (Apple Silicon GPU) available\")\n",
    "    torch.mps.set_per_process_memory_fraction(0.8)  # GPU ë©”ëª¨ë¦¬ ì œí•œ\n",
    "else:\n",
    "    print(\"âŒ MPS not available, using CPU\")\n",
    "\n",
    "# Keras import (ë°±ì—”ë“œ í™•ì¸)\n",
    "import keras\n",
    "print(f\"ğŸ¯ Keras backend: {keras.backend.backend()}\")\n",
    "\n",
    "# TensorFlowëŠ” importí•˜ì§€ ì•ŠìŒ!\n",
    "import keras_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516ff87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ë¡œë“œë§Œ\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e3f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma model loaded with PyTorch backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 19:58:19.038206: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Max\n",
      "2025-08-19 19:58:19.038282: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2025-08-19 19:58:19.038287: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2025-08-19 19:58:19.038295: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-08-19 19:58:19.038305: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch ë°±ì—”ë“œë¡œ Gemma ëª¨ë¸ ë¡œë”©\n",
    "gemma = keras_hub.models.GemmaCausalLM.from_preset('gemma_2b_en')\n",
    "\n",
    "print(\"Gemma model loaded with PyTorch backend\")\n",
    "# gemma.summary()  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e903d790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay foolish\n",
      "\n",
      "<h1>#stayhungrystayfoolish</h1>\n",
      "\n",
      "<strong>What does this\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gemma.compile(sampler=sampler)\n",
    "\n",
    "# PyTorch ë°±ì—”ë“œë¡œ í…ìŠ¤íŠ¸ ìƒì„± (MPS GPU ìë™ ì‚¬ìš©)\n",
    "text = gemma.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a6f64d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë´„ì´ ì˜¤ë©´ì„œ ê±´ì¡°í•œ íë¦° ë‚ ì”¨ê°€ ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. \n"
     ]
    }
   ],
   "source": [
    "text = gemma.generate('ë´„ì´ ì˜¤ë©´', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb63b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a20ce6d1dc744f4b39022ce48db989a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'ë´„ì´ ì˜¤ë©´ì„œ ê²¨ìš¸ì´ ë¹¨ë¦¬ ì§€ë‚˜ê°€ëŠ” ê²ƒ ê°™ì•„'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "gemma_pipe = pipeline('text-generation', model='beomi/gemma-ko-2b')\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "text = gemma_pipe('ë´„ì´ ì˜¤ë©´', max_length=20, truncation=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110eec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay foolish.\n",
      "\n",
      "i'm excited for 2019! i'm looking forward to being more consistent with my\n"
     ]
    }
   ],
   "source": [
    "gemma = keras_hub.models.GemmaCausalLM.from_preset('gemma2_2b_en')\n",
    "\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gemma.compile(sampler=sampler)\n",
    "\n",
    "text = gemma.generate('stay hungry, stay', max_length=30)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb432e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay foolish\n",
      "\n",
      "I have been trying to get the past few months to go back\n"
     ]
    }
   ],
   "source": [
    "#gemma = keras_hub.models.Gemma3CausalLM.from_preset('gemma3_12b')\n",
    "gemma = keras_hub.models.Gemma3CausalLM.from_preset('gemma3_4b')\n",
    "\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gemma.compile(sampler=sampler)\n",
    "\n",
    "text = gemma.generate('stay hungry, stay', max_length=20)\n",
    "print(text)\n",
    "\n",
    "# gemma.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532afc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay hungry<start_of_image><start_of_image>ã€€ã€€åŒ—äº¬æ—¶é—´7æœˆ27\n"
     ]
    }
   ],
   "source": [
    "gemma = keras_hub.models.Gemma3CausalLM.from_preset('gemma3_270m')\n",
    "\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gemma.compile(sampler=sampler)\n",
    "\n",
    "text = gemma.generate('stay hungry, stay', max_length=20)\n",
    "print(text)\n",
    "\n",
    "# gemma.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
