{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b0f535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MPS (Apple Silicon GPU) available\n",
      "🎯 Keras backend: torch\n"
     ]
    }
   ],
   "source": [
    "# 반드시 첫 번째 셀에서 백엔드 설정\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "\n",
    "# PyTorch MPS 확인\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"✅ MPS (Apple Silicon GPU) available\")\n",
    "else:\n",
    "    print(\"❌ MPS not available, using CPU\")\n",
    "\n",
    "# Keras import (백엔드 확인)\n",
    "import keras\n",
    "print(f\"🎯 Keras backend: {keras.backend.backend()}\")\n",
    "\n",
    "# TensorFlow는 import하지 않음!\n",
    "import keras_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be1e399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933acb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 20:09:32.082054: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Max\n",
      "2025-08-19 20:09:32.082100: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2025-08-19 20:09:32.082106: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2025-08-19 20:09:32.082118: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-08-19 20:09:32.082136: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gpt2_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                                │                       Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gpt2_tokenizer (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                                │                       Vocab size: \u001b[38;5;34m50,257\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt2_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gpt2_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │ gpt2_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gpt2_backbone (\u001b[38;5;33mGPT2Backbone\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)         │     \u001b[38;5;34m124,439,808\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)       │      \u001b[38;5;34m38,597,376\u001b[0m │ gpt2_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gpt2_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                                │                       Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gpt2_tokenizer (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                                │                       Vocab size: \u001b[38;5;34m50,257\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt2_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gpt2_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │ gpt2_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gpt2_backbone (\u001b[38;5;33mGPT2Backbone\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)         │     \u001b[38;5;34m124,439,808\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)       │      \u001b[38;5;34m38,597,376\u001b[0m │ gpt2_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2 = keras_hub.models.GPT2CausalLM.from_preset(\"gpt2_base_en\")\n",
    "gpt2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8f1bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay thirsty\n"
     ]
    }
   ],
   "source": [
    "text = gpt2.generate('stay hungry, stay', max_length=6)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca30aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay thirsty, stay tired of your own life.\n",
      "\n",
      "You know what's\n"
     ]
    }
   ],
   "source": [
    "text = gpt2.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b034f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'token_ids': tensor([50256, 31712, 14720,    11,  2652, 50256,     0,     0,     0,     0],\n",
       "         device='mps:0', dtype=torch.int32),\n",
       "  'padding_mask': tensor([ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "         device='mps:0')},\n",
       " tensor([31712, 14720,    11,  2652, 50256,     0,     0,     0,     0,     0],\n",
       "        device='mps:0', dtype=torch.int32),\n",
       " tensor([ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "        device='mps:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, target, mask = gpt2.preprocessor('stay hungry, stay', sequence_length=10)\n",
    "\n",
    "inputs, target, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e934a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay Ġhungry , Ġstay <|endoftext|> ! ! ! ! ! "
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = gpt2.preprocessor.tokenizer\n",
    "\n",
    "for ids in target:\n",
    "    print(gpt2_tokenizer.id_to_token(ids), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fed080d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': tensor([[50256, 31712, 14720,    11,  2652,     0,     0,     0,     0,     0]],\n",
       "        device='mps:0', dtype=torch.int32),\n",
       " 'padding_mask': tensor([[ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
       "        device='mps:0')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = gpt2.preprocessor.generate_preprocess(['stay hungry, stay'], sequence_length=10)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295d3921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': tensor([[50256, 31712, 14720,    11,  2652,  5448,    13,   198,   198,  1135]],\n",
       "        device='mps:0', dtype=torch.int32),\n",
       " 'padding_mask': tensor([[True, True, True, True, True, True, True, True, True, True]],\n",
       "        device='mps:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = gpt2.generate_function(inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef31a3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stay hungry, stay healthy.\\n\\nWe']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.preprocessor.generate_postprocess(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12ca95a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stay hungry, stay warm, stay warm\\n\\nstay in your comfort zone and stay in the'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopKSampler(k=10, temperature=0.5, seed=42)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "gpt2.generate('stay hungry, stay', max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf7236f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stay hungry, stay warm -\\n\"The last three years has been a roller-skier'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopKSampler(k=10, temperature=5, seed=42)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "gpt2.generate('stay hungry, stay', max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d0bc5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stay hungry, stay thirsty: The first few years of a new era are filled by foodies'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpt2.generate(tf.constant('stay hungry, stay'), max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac18421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stay hungry, stay warm\\n\\ncoffee : ice cold not iced coffee\\n\\nch'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fa5bf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stay hungry, stay warm\". Of myself rather: An that sounds . Aw many beeth whales'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, k=1000, temperature=5, seed=42)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d56a1702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay\n"
     ]
    }
   ],
   "source": [
    "gpt2.compile(sampler='top_p')\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1dd1213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay\n"
     ]
    }
   ],
   "source": [
    "gpt2.compile(sampler='greedy')\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbfaf589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay jacket Well Feesal negativeHub█ registered Fingeronel lazyRecord six athleticism Carly\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "\n",
    "sampler = keras_hub.samplers.RandomSampler(temperature=5, seed=42)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "685a5876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay hydrated stay hydrated\n",
      "\n",
      "Stay hydrated stay hydrated\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "\n",
    "sampler = keras_hub.samplers.BeamSampler(num_beams=10, temperature=5)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc72c6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "\n",
    "sampler = keras_hub.samplers.ContrastiveSampler(k=10, alpha=0.2)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "484f4f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay thirsty\n",
      "\n",
      "\n",
      "A lot has changed in the last 20 years. There are\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "\n",
    "sampler = keras_hub.samplers.ContrastiveSampler(k=10, alpha=0.8)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68ba325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"stay hungry, stay dry. sometimes the temperature drops even farther down into a freezing winter and it's\"}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "hf_gpt1 = pipeline('text-generation', model='openai-community/openai-gpt')\n",
    "\n",
    "hf_gpt1('stay hungry, stay', max_length=20, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca9f8468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"stay hungry, stay dry. sometimes the temperature drops even farther down into a freezing winter and it's\"},\n",
       " {'generated_text': \"stay hungry, stay out of my way and keep it in. if i were you, i 'd\"},\n",
       " {'generated_text': 'stay hungry, stay out of trouble, get some more food in your bellies, and maybe some real'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "hf_gpt1('stay hungry, stay', max_length=20, truncation=True, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0c9fc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[31712, 14720,    11,  2652]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "hf_gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "hf_gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "prep_data = hf_gpt2_tokenizer('stay hungry, stay', return_tensors='pt')\n",
    "prep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d37a52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31712, 14720,    11,  2652, 47124,    11,  2652, 47124,    11,  2652,\n",
      "         47124,    11,  2652, 47124,    11,  2652, 47124,    11,  2652, 47124]])\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "hf_gpt2_model.generation_config.pad_token_id = hf_gpt2_model.generation_config.eos_token_id\n",
    "\n",
    "prep_data = hf_gpt2_tokenizer('stay hungry, stay', return_tensors='pt')\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fca5be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stay hungry, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_gpt2_tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef36af02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stay hungry, stay thirsty and stay a little drunk in hot spots, stay in the kitchen and then']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20, do_sample=True)\n",
    "\n",
    "hf_gpt2_tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d5a2ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stay hungry, stay with mom,\" a female reporter in Toronto tells him . And to see something from']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20, do_sample=True, top_p=0.8, temperature=5.0)\n",
    "\n",
    "hf_gpt2_tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d40eedb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stay hungry, stay hungry, stay hungry, stay hungry, stay hungry, stay hungry, stay hungry']\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20, num_beams=5)\n",
    "\n",
    "text = hf_gpt2_tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d96befd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stay hungry, stay in shape or get some rest for your kids. Your family will know you have']\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20, num_beams=5, top_k=20, do_sample=True, temperature=5.0)\n",
    "\n",
    "text = hf_gpt2_tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e57d588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"stay hungry, stay out of trouble\\n\\n\\nDon't want us to be able to do that?\"]\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20, penalty_alpha=0.8)\n",
    "\n",
    "text = hf_gpt2_tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d74351e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 20,\n",
       " 'max_new_tokens': None,\n",
       " 'min_length': 0,\n",
       " 'min_new_tokens': None,\n",
       " 'early_stopping': False,\n",
       " 'max_time': None,\n",
       " 'stop_strings': None,\n",
       " 'do_sample': False,\n",
       " 'num_beams': 1,\n",
       " 'num_beam_groups': 1,\n",
       " 'penalty_alpha': None,\n",
       " 'dola_layers': None,\n",
       " 'use_cache': True,\n",
       " 'cache_implementation': None,\n",
       " 'cache_config': None,\n",
       " 'return_legacy_cache': None,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'min_p': None,\n",
       " 'typical_p': 1.0,\n",
       " 'epsilon_cutoff': 0.0,\n",
       " 'eta_cutoff': 0.0,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'encoder_repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'force_words_ids': None,\n",
       " 'renormalize_logits': False,\n",
       " 'constraints': None,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'forced_decoder_ids': None,\n",
       " 'sequence_bias': None,\n",
       " 'token_healing': False,\n",
       " 'guidance_scale': None,\n",
       " 'low_memory': None,\n",
       " 'watermarking_config': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'output_attentions': False,\n",
       " 'output_hidden_states': False,\n",
       " 'output_scores': False,\n",
       " 'output_logits': None,\n",
       " 'return_dict_in_generate': False,\n",
       " 'pad_token_id': None,\n",
       " 'bos_token_id': None,\n",
       " 'eos_token_id': None,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'decoder_start_token_id': None,\n",
       " 'is_assistant': False,\n",
       " 'num_assistant_tokens': 20,\n",
       " 'num_assistant_tokens_schedule': 'constant',\n",
       " 'assistant_confidence_threshold': 0.4,\n",
       " 'prompt_lookup_num_tokens': None,\n",
       " 'max_matching_ngram_size': None,\n",
       " 'assistant_early_exit': None,\n",
       " 'assistant_lookbehind': 10,\n",
       " 'target_lookbehind': 10,\n",
       " 'generation_kwargs': {},\n",
       " '_from_model_config': False,\n",
       " 'transformers_version': '4.47.1'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "GenerationConfig().to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
