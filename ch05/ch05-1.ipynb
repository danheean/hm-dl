{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b0f535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MPS (Apple Silicon GPU) available\n",
      "🎯 Keras backend: torch\n"
     ]
    }
   ],
   "source": [
    "# 반드시 첫 번째 셀에서 백엔드 설정\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "\n",
    "# PyTorch MPS 확인\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"✅ MPS (Apple Silicon GPU) available\")\n",
    "else:\n",
    "    print(\"❌ MPS not available, using CPU\")\n",
    "\n",
    "# Keras import (백엔드 확인)\n",
    "import keras\n",
    "print(f\"🎯 Keras backend: {keras.backend.backend()}\")\n",
    "\n",
    "# TensorFlow는 import하지 않음!\n",
    "import keras_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be1e399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07555a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocal_size = 50257\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "hidden_dim = 768\n",
    "dropout = 0.1\n",
    "activation = 'gelu'\n",
    "max_seq_len = 1024\n",
    "\n",
    "token_ids = keras.Input(shape=(None, ))\n",
    "padding_mask = keras.Input(shape=(None, ))\n",
    "\n",
    "def make_causal_mask(seq_len):\n",
    "    n_hori = keras.ops.arange(seq_len)\n",
    "    n_vert = keras.ops.expand_dims(n_hori, axis=-1)\n",
    "    mask = n_vert >= n_hori\n",
    "    return mask\n",
    "\n",
    "def make_attention_mask(padding_mask):\n",
    "    batch_size, seq_len = keras.ops.shape(padding_mask)\n",
    "    causal_mask = make_causal_mask(seq_len)\n",
    "    causal_mask = keras.ops.broadcast_to(causal_mask, (batch_size, seq_len, seq_len))\n",
    "    attention_mask = keras.ops.expand_dims(padding_mask, axis=1)\n",
    "    return keras.ops.minimum(causal_mask, attention_mask)\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "class AttentionMask(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, padding_mask):\n",
    "        return make_attention_mask(padding_mask)\n",
    "\n",
    "def transformer_docoder(x, padding_mask, dropout, activation='relu', norm_first=True):\n",
    "    # 1. 어텐션 마스크를 계산\n",
    "    attention_mask = AttentionMask()(padding_mask)\n",
    "\n",
    "    # 2. 스킵 연결 중비\n",
    "    residual = x\n",
    "    key_dim = hidden_dim // num_heads\n",
    "    if norm_first:\n",
    "        x = keras.layers.LayerNormalization()(x)\n",
    "\n",
    "    # 3. 멀티 헤드 어텐션 통과\n",
    "    x = layers.MultiHeadAttention(num_heads, key_dim, droupout=dropout)(query=x, value=x, attention_mask=attention_mask)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    # 4. 스킵 연결\n",
    "    x = x + residual\n",
    "    if not norm_first:\n",
    "        x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # 5. 스킵 연결 중비\n",
    "    residual = x\n",
    "\n",
    "    # 6. 위치별 피드 포워드 네트워크\n",
    "    if norm_first:\n",
    "        x = layers.LayerNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(hidden_dim * 4, activation=activation)(x)\n",
    "    x = layers.Denst(hidden_dim)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    # 7. 스킵 연결\n",
    "    x = x + residual\n",
    "    if not norm_first:\n",
    "        x = layers.LayerNormalization()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933acb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 13:54:21.472340: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Max\n",
      "2025-08-20 13:54:21.472414: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2025-08-20 13:54:21.472420: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2025-08-20 13:54:21.472438: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-08-20 13:54:21.472459: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gpt2_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                                │                       Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gpt2_tokenizer (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                                │                       Vocab size: \u001b[38;5;34m50,257\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt2_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gpt2_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │ gpt2_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gpt2_backbone (\u001b[38;5;33mGPT2Backbone\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)         │     \u001b[38;5;34m124,439,808\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)       │      \u001b[38;5;34m38,597,376\u001b[0m │ gpt2_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2 = keras_hub.models.GPT2CausalLM.from_preset(\"gpt2_base_en\")\n",
    "gpt2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8f1bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay thirsty\n"
     ]
    }
   ],
   "source": [
    "text = gpt2.generate('stay hungry, stay', max_length=6)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca30aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay healthy!\n",
      "\n",
      "\n",
      "This is an easy, quick, and easy recipe that\n"
     ]
    }
   ],
   "source": [
    "text = gpt2.generate('stay hungry, stay', max_length=20)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b034f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'token_ids': tensor([50256, 31712, 14720,    11,  2652, 50256,     0,     0,     0,     0],\n",
       "         device='mps:0', dtype=torch.int32),\n",
       "  'padding_mask': tensor([ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "         device='mps:0')},\n",
       " tensor([31712, 14720,    11,  2652, 50256,     0,     0,     0,     0,     0],\n",
       "        device='mps:0', dtype=torch.int32),\n",
       " tensor([ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "        device='mps:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, target, mask = gpt2.preprocessor('stay hungry, stay', sequence_length=10)\n",
    "\n",
    "inputs, target, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e934a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay Ġhungry , Ġstay <|endoftext|> ! ! ! ! ! "
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = gpt2.preprocessor.tokenizer\n",
    "\n",
    "for ids in target:\n",
    "    print(gpt2_tokenizer.id_to_token(ids), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fed080d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': tensor([[50256, 31712, 14720,    11,  2652,     0,     0,     0,     0,     0]],\n",
       "        device='mps:0', dtype=torch.int32),\n",
       " 'padding_mask': tensor([[ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
       "        device='mps:0')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = gpt2.preprocessor.generate_preprocess(['stay hungry, stay'], sequence_length=10)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295d3921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': tensor([[50256, 31712, 14720,    11,  2652,  5448,    13,   198,   198,  1135]],\n",
       "        device='mps:0', dtype=torch.int32),\n",
       " 'padding_mask': tensor([[True, True, True, True, True, True, True, True, True, True]],\n",
       "        device='mps:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = gpt2.generate_function(inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef31a3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stay hungry, stay healthy.\\n\\nWe']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.preprocessor.generate_postprocess(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ca95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay warm, stay warm\n",
      "\n",
      "stay in your comfort zone and stay in the moment\n",
      "\n",
      "stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay in the moment stay\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopKSampler(k=10, temperature=0.5, seed=42)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=100)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bf7236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay warm -\n",
      "\"The last three years has been a roller-skier ride and we all look out for ourselves and each day that comes to our attention can bring out all manner of negative emotion,\" he continued. \"(A little more of an emotional roller skydiver will come up) so I hope our next move will bring our attention.\" He is now a volunteer on an island on Fiji's Pacific Island nation.\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopKSampler(k=10, temperature=5, seed=42)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=100)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d0bc5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stay hungry, stay thirsty: The first few years of a new era are filled by foodies'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpt2.generate(tf.constant('stay hungry, stay'), max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dac18421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay warm\n",
      "\n",
      "coffee : ice cold not iced coffee\n",
      "\n",
      "chocolate : not iced chocolate\n",
      "\n",
      "tofu : not iced tofu\n",
      "\n",
      "basil : not iced cafe\n",
      "\n",
      "mooch : not iced mooch\n",
      "\n",
      "mehtee : not iced mehtee\n",
      "\n",
      "coconut : not iced coconut\n",
      "\n",
      "cranberry : not iced cranberry\n",
      "\n",
      "parfa :\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, seed=42)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=100)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fa5bf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stay hungry, stay warm\". Of myself rather: An that sounds . Aw many beeth whales yet told 6 d . that shows 1 br.. human people stop hearing help 21 asshraces both rats ll will come 14 hit not injured W på ar ? utfð out! leave om on 60 mins isav somnme legmannks swiedmar that werett gende fail! had W0 S77 thru grufr maroon 2 writched 03 served ship av'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopPSampler(p=0.8, k=1000, temperature=5, seed=42)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=100)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d56a1702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty,\n"
     ]
    }
   ],
   "source": [
    "gpt2.compile(sampler='top_p')\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=100)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1dd1213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty,\n"
     ]
    }
   ],
   "source": [
    "gpt2.compile(sampler='greedy')\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=100)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbfaf589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay jacket Well Feesal negativeHub█ registered Fingeronel lazyRecord six athleticism Carlyociate StefanicusBel presentswe Birthday Suzanne appreillonISHrecentaku insurance Og Yizzle Testing Toy given 600xi ratedRFietal AST mileage commercially contradicted Wald2012エル@@@@ Yin Bulls reinforcementfs Haz unquestioniries fri plays awhile HanAH Qatar Lakshrt683ifiers Const TN dinnerisc Sara NSAarius inches Orth plate construed feather MHzasty Idaho foundEED bringing crossed foreseeable dissu mattobos crystal Sent Bhar PROMag beginnerSometimes\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "\n",
    "sampler = keras_hub.samplers.RandomSampler(temperature=5, seed=42)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=100)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "685a5876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay hydrated stay hydrated\n",
      "\n",
      "Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hydrated Stay hyd\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "\n",
    "sampler = keras_hub.samplers.BeamSampler(num_beams=10, temperature=5)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=200)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc72c6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty,\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "\n",
    "sampler = keras_hub.samplers.ContrastiveSampler(k=1000, alpha=0.2)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=100)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "484f4f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay hungry, stay part of the\n",
      "\n",
      "\n",
      "wwwnodetics.com Quote of the Day Mr. Potato Head co-founder known enough to self-acquaint himself with Coca-Cola pic.twitter.deviantart.com/wp-cgi-bin/2013/featuring-Donald-Gary-of-Pennsylvania Pennsylvanian sort of pic.twitter.com/qYqYqYqwo www twitchla.mit.edu(AP) 800-STEE\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "\n",
    "sampler = keras_hub.samplers.ContrastiveSampler(k=1000, alpha=0.8)\n",
    "\n",
    "gpt2.compile(sampler=sampler)\n",
    "\n",
    "text = gpt2.generate('stay hungry, stay', max_length=100)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68ba325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"stay hungry, stay dry. sometimes the temperature drops even farther down into a freezing winter and it's\"}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "hf_gpt1 = pipeline('text-generation', model='openai-community/openai-gpt')\n",
    "\n",
    "hf_gpt1('stay hungry, stay', max_length=20, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca9f8468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"stay hungry, stay dry. sometimes the temperature drops even farther down into a freezing winter and it's\"},\n",
       " {'generated_text': \"stay hungry, stay out of my way and keep it in. if i were you, i 'd\"},\n",
       " {'generated_text': 'stay hungry, stay out of trouble, get some more food in your bellies, and maybe some real'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "hf_gpt1('stay hungry, stay', max_length=20, truncation=True, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0c9fc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[31712, 14720,    11,  2652]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "hf_gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "hf_gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "prep_data = hf_gpt2_tokenizer('stay hungry, stay', return_tensors='pt')\n",
    "prep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d37a52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31712, 14720,    11,  2652, 47124,    11,  2652, 47124,    11,  2652,\n",
      "         47124,    11,  2652, 47124,    11,  2652, 47124,    11,  2652, 47124]])\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "hf_gpt2_model.generation_config.pad_token_id = hf_gpt2_model.generation_config.eos_token_id\n",
    "\n",
    "prep_data = hf_gpt2_tokenizer('stay hungry, stay', return_tensors='pt')\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fca5be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stay hungry, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty, stay thirsty']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_gpt2_tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef36af02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stay hungry, stay thirsty and stay a little drunk in hot spots, stay in the kitchen and then']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20, do_sample=True)\n",
    "\n",
    "hf_gpt2_tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d5a2ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stay hungry, stay with mom,\" a female reporter in Toronto tells him . And to see something from']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20, do_sample=True, top_p=0.8, temperature=5.0)\n",
    "\n",
    "hf_gpt2_tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d40eedb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stay hungry, stay hungry, stay hungry, stay hungry, stay hungry, stay hungry, stay hungry']\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20, num_beams=5)\n",
    "\n",
    "text = hf_gpt2_tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d96befd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stay hungry, stay in shape or get some rest for your kids. Your family will know you have']\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20, num_beams=5, top_k=20, do_sample=True, temperature=5.0)\n",
    "\n",
    "text = hf_gpt2_tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e57d588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"stay hungry, stay out of trouble\\n\\n\\nDon't want us to be able to do that?\"]\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "outputs = hf_gpt2_model.generate(**prep_data, max_length=20, penalty_alpha=0.8)\n",
    "\n",
    "text = hf_gpt2_tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d74351e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 20,\n",
       " 'max_new_tokens': None,\n",
       " 'min_length': 0,\n",
       " 'min_new_tokens': None,\n",
       " 'early_stopping': False,\n",
       " 'max_time': None,\n",
       " 'stop_strings': None,\n",
       " 'do_sample': False,\n",
       " 'num_beams': 1,\n",
       " 'num_beam_groups': 1,\n",
       " 'penalty_alpha': None,\n",
       " 'dola_layers': None,\n",
       " 'use_cache': True,\n",
       " 'cache_implementation': None,\n",
       " 'cache_config': None,\n",
       " 'return_legacy_cache': None,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'min_p': None,\n",
       " 'typical_p': 1.0,\n",
       " 'epsilon_cutoff': 0.0,\n",
       " 'eta_cutoff': 0.0,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'encoder_repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'force_words_ids': None,\n",
       " 'renormalize_logits': False,\n",
       " 'constraints': None,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'forced_decoder_ids': None,\n",
       " 'sequence_bias': None,\n",
       " 'token_healing': False,\n",
       " 'guidance_scale': None,\n",
       " 'low_memory': None,\n",
       " 'watermarking_config': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'output_attentions': False,\n",
       " 'output_hidden_states': False,\n",
       " 'output_scores': False,\n",
       " 'output_logits': None,\n",
       " 'return_dict_in_generate': False,\n",
       " 'pad_token_id': None,\n",
       " 'bos_token_id': None,\n",
       " 'eos_token_id': None,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'decoder_start_token_id': None,\n",
       " 'is_assistant': False,\n",
       " 'num_assistant_tokens': 20,\n",
       " 'num_assistant_tokens_schedule': 'constant',\n",
       " 'assistant_confidence_threshold': 0.4,\n",
       " 'prompt_lookup_num_tokens': None,\n",
       " 'max_matching_ngram_size': None,\n",
       " 'assistant_early_exit': None,\n",
       " 'assistant_lookbehind': 10,\n",
       " 'target_lookbehind': 10,\n",
       " 'generation_kwargs': {},\n",
       " '_from_model_config': False,\n",
       " 'transformers_version': '4.47.1'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "GenerationConfig().to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
